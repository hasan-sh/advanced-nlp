{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f811453b-928b-4b65-a803-b2c6437d0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc11351-1e8f-410e-9a4c-3159c07a8f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hisha\\AppData\\Local\\Temp\\ipykernel_27256\\1253375658.py:3: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(train_file, delimiter='\\t')\n",
      "C:\\Users\\Hisha\\AppData\\Local\\Temp\\ipykernel_27256\\1253375658.py:4: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_df = pd.read_csv(test_file, delimiter='\\t')\n"
     ]
    }
   ],
   "source": [
    "train_file = 'data/train_ner.tsv'\n",
    "test_file = 'data/test_ner.tsv'\n",
    "train_df = pd.read_csv(train_file, delimiter='\\t')\n",
    "test_df = pd.read_csv(test_file, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c54f5f1-3e1c-4c74-bd5b-adb547960c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df[\"distance_head\"]!=\"_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5539fc94-30b3-4f15-ac36-21264f4bfbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0219090a-7aa6-4f49-8612-7e906577c9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>uni_POS</th>\n",
       "      <th>morph_type</th>\n",
       "      <th>distance_head</th>\n",
       "      <th>dep_label</th>\n",
       "      <th>dep_rel</th>\n",
       "      <th>space</th>\n",
       "      <th>probbank</th>\n",
       "      <th>ner</th>\n",
       "      <th>token_bigram</th>\n",
       "      <th>pos_bigram</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Al</td>\n",
       "      <td>Al</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Number=Sing</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>0:root</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "      <td>_</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>(None, 'Al')</td>\n",
       "      <td>(None, 'PROPN')</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>HYPH</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>punct</td>\n",
       "      <td>1:punct</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "      <td>_</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>('Al', '-')</td>\n",
       "      <td>('PROPN', 'PUNCT')</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Zaman</td>\n",
       "      <td>Zaman</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Number=Sing</td>\n",
       "      <td>1</td>\n",
       "      <td>flat</td>\n",
       "      <td>1:flat</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>('-', 'Zaman')</td>\n",
       "      <td>('PUNCT', 'PROPN')</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>:</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>punct</td>\n",
       "      <td>1:punct</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>('Zaman', ':')</td>\n",
       "      <td>('PROPN', 'PUNCT')</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>American</td>\n",
       "      <td>american</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Degree=Pos</td>\n",
       "      <td>6</td>\n",
       "      <td>amod</td>\n",
       "      <td>6:amod</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>NORP</td>\n",
       "      <td>(':', 'American')</td>\n",
       "      <td>('PUNCT', 'ADJ')</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035899</th>\n",
       "      <td>1035923</td>\n",
       "      <td>7506</td>\n",
       "      <td>131.0</td>\n",
       "      <td>graduated</td>\n",
       "      <td>graduate</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>Tense=Past|VerbForm=Part</td>\n",
       "      <td>123</td>\n",
       "      <td>advcl</td>\n",
       "      <td>123:advcl:since</td>\n",
       "      <td>_</td>\n",
       "      <td>graduate.01</td>\n",
       "      <td>_</td>\n",
       "      <td>('have', 'graduated')</td>\n",
       "      <td>('AUX', 'VERB')</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035900</th>\n",
       "      <td>1035924</td>\n",
       "      <td>7506</td>\n",
       "      <td>132.0</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>_</td>\n",
       "      <td>134</td>\n",
       "      <td>cc</td>\n",
       "      <td>134:cc</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>('graduated', 'and')</td>\n",
       "      <td>('VERB', 'CCONJ')</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035901</th>\n",
       "      <td>1035925</td>\n",
       "      <td>7506</td>\n",
       "      <td>133.0</td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Case=Nom|Number=Sing|Person=1|PronType=Prs</td>\n",
       "      <td>134</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>134:nsubj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>('and', 'i')</td>\n",
       "      <td>('CCONJ', 'PRON')</td>\n",
       "      <td>ARG0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035902</th>\n",
       "      <td>1035926</td>\n",
       "      <td>7506</td>\n",
       "      <td>134.0</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBP</td>\n",
       "      <td>Mood=Ind|Tense=Pres|VerbForm=Fin</td>\n",
       "      <td>4</td>\n",
       "      <td>conj</td>\n",
       "      <td>4:conj:and</td>\n",
       "      <td>_</td>\n",
       "      <td>hate.01</td>\n",
       "      <td>_</td>\n",
       "      <td>('i', 'hate')</td>\n",
       "      <td>('PRON', 'VERB')</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035903</th>\n",
       "      <td>1035927</td>\n",
       "      <td>7506</td>\n",
       "      <td>135.0</td>\n",
       "      <td>drama</td>\n",
       "      <td>drama</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>Number=Sing</td>\n",
       "      <td>134</td>\n",
       "      <td>obj</td>\n",
       "      <td>134:obj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>('hate', 'drama')</td>\n",
       "      <td>('VERB', 'NOUN')</td>\n",
       "      <td>ARG1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1035904 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index  sent_id  token_id      token     lemma    POS uni_POS  \\\n",
       "0              0        0       1.0         Al        Al  PROPN     NNP   \n",
       "1              1        0       2.0          -         -  PUNCT    HYPH   \n",
       "2              2        0       3.0      Zaman     Zaman  PROPN     NNP   \n",
       "3              3        0       4.0          :         :  PUNCT       :   \n",
       "4              4        0       5.0   American  american    ADJ      JJ   \n",
       "...          ...      ...       ...        ...       ...    ...     ...   \n",
       "1035899  1035923     7506     131.0  graduated  graduate   VERB     VBN   \n",
       "1035900  1035924     7506     132.0        and       and  CCONJ      CC   \n",
       "1035901  1035925     7506     133.0          i         i   PRON     PRP   \n",
       "1035902  1035926     7506     134.0       hate      hate   VERB     VBP   \n",
       "1035903  1035927     7506     135.0      drama     drama   NOUN      NN   \n",
       "\n",
       "                                         morph_type distance_head dep_label  \\\n",
       "0                                       Number=Sing             0      root   \n",
       "1                                                 _             1     punct   \n",
       "2                                       Number=Sing             1      flat   \n",
       "3                                                 _             1     punct   \n",
       "4                                        Degree=Pos             6      amod   \n",
       "...                                             ...           ...       ...   \n",
       "1035899                    Tense=Past|VerbForm=Part           123     advcl   \n",
       "1035900                                           _           134        cc   \n",
       "1035901  Case=Nom|Number=Sing|Person=1|PronType=Prs           134     nsubj   \n",
       "1035902            Mood=Ind|Tense=Pres|VerbForm=Fin             4      conj   \n",
       "1035903                                 Number=Sing           134       obj   \n",
       "\n",
       "                 dep_rel          space     probbank     ner  \\\n",
       "0                 0:root  SpaceAfter=No            _  PERSON   \n",
       "1                1:punct  SpaceAfter=No            _  PERSON   \n",
       "2                 1:flat              _            _  PERSON   \n",
       "3                1:punct              _            _       _   \n",
       "4                 6:amod              _            _    NORP   \n",
       "...                  ...            ...          ...     ...   \n",
       "1035899  123:advcl:since              _  graduate.01       _   \n",
       "1035900           134:cc              _            _       _   \n",
       "1035901        134:nsubj              _            _  PERSON   \n",
       "1035902       4:conj:and              _      hate.01       _   \n",
       "1035903          134:obj              _            _       _   \n",
       "\n",
       "                  token_bigram          pos_bigram target  \n",
       "0                 (None, 'Al')     (None, 'PROPN')      _  \n",
       "1                  ('Al', '-')  ('PROPN', 'PUNCT')      _  \n",
       "2               ('-', 'Zaman')  ('PUNCT', 'PROPN')      _  \n",
       "3               ('Zaman', ':')  ('PROPN', 'PUNCT')      _  \n",
       "4            (':', 'American')    ('PUNCT', 'ADJ')      _  \n",
       "...                        ...                 ...    ...  \n",
       "1035899  ('have', 'graduated')     ('AUX', 'VERB')      _  \n",
       "1035900   ('graduated', 'and')   ('VERB', 'CCONJ')      _  \n",
       "1035901           ('and', 'i')   ('CCONJ', 'PRON')   ARG0  \n",
       "1035902          ('i', 'hate')    ('PRON', 'VERB')      V  \n",
       "1035903      ('hate', 'drama')    ('VERB', 'NOUN')   ARG1  \n",
       "\n",
       "[1035904 rows x 17 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dda8be51-057c-47e7-8a7b-148edd46c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the categorical and numerical features\n",
    "cat_features = train_df[['token', 'lemma', 'POS', 'uni_POS', 'morph_type',\n",
    "                     'dep_label', 'dep_rel', 'space', 'probbank', 'ner',\n",
    "                     'token_bigram', 'pos_bigram']]\n",
    "num_features = train_df[['sent_id', 'token_id', 'distance_head']]\n",
    "num_features = num_features.astype(int)\n",
    "# Apply the hashing trick to the categorical features\n",
    "hasher = FeatureHasher(n_features=12, input_type='string')\n",
    "hashed_features = hasher.transform(cat_features.values.astype(str))\n",
    "\n",
    "# Convert the resulting sparse matrix to a dense matrix and concatenate with the numerical features\n",
    "hashed_features = pd.DataFrame(hashed_features.toarray())\n",
    "X = pd.concat([hashed_features, num_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9772e014-eedf-4ed4-9fa1-7fda12c4d3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>distance_head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035899</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7506</td>\n",
       "      <td>131</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035900</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7506</td>\n",
       "      <td>132</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035901</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7506</td>\n",
       "      <td>133</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035902</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7506</td>\n",
       "      <td>134</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035903</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7506</td>\n",
       "      <td>135</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1035904 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0    1    2    3    4    5    6    7    8    9   10   11  sent_id  \\\n",
       "0       -2.0  1.0 -1.0  2.0  0.0  0.0 -1.0  0.0 -1.0  1.0 -1.0  0.0        0   \n",
       "1        0.0 -1.0  0.0  3.0  0.0  0.0 -1.0  0.0  0.0  1.0  0.0  0.0        0   \n",
       "2       -1.0  0.0 -1.0  2.0  0.0 -2.0 -1.0  0.0  0.0  1.0  1.0  1.0        0   \n",
       "3        0.0  1.0  3.0  4.0  0.0  1.0  0.0 -1.0  0.0  0.0  0.0  0.0        0   \n",
       "4        1.0  1.0 -1.0  1.0  0.0  0.0  0.0  0.0 -1.0  0.0  2.0  1.0        0   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...      ...   \n",
       "1035899  0.0  1.0  1.0  2.0  0.0 -1.0  1.0  0.0  0.0 -1.0  1.0  0.0     7506   \n",
       "1035900  0.0  0.0 -1.0  4.0  0.0 -1.0  0.0  1.0  0.0  1.0  0.0  0.0     7506   \n",
       "1035901 -1.0  1.0  0.0  2.0 -1.0  0.0 -1.0  1.0  0.0 -3.0  1.0  1.0     7506   \n",
       "1035902  0.0  1.0  0.0  2.0 -2.0  1.0  0.0  0.0 -2.0  0.0  0.0 -2.0     7506   \n",
       "1035903  0.0  1.0  1.0  3.0  0.0 -2.0  0.0  0.0  2.0  1.0  0.0  0.0     7506   \n",
       "\n",
       "         token_id  distance_head  \n",
       "0               1              0  \n",
       "1               2              1  \n",
       "2               3              1  \n",
       "3               4              1  \n",
       "4               5              6  \n",
       "...           ...            ...  \n",
       "1035899       131            123  \n",
       "1035900       132            134  \n",
       "1035901       133            134  \n",
       "1035902       134              4  \n",
       "1035903       135            134  \n",
       "\n",
       "[1035904 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85467105-17fd-4687-ad3e-35e2640e8b26",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23fcb69d-88b2-498b-8fc3-cde1bf15e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.assign(label=[0 if target==\"_\" or target==\"V\" else 1 for target in train_df['target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab68bbf-4fdc-4c8f-9d7f-ec4d735a9094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "          ..\n",
       "1035899    0\n",
       "1035900    0\n",
       "1035901    1\n",
       "1035902    0\n",
       "1035903    1\n",
       "Name: label, Length: 1035904, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3592eea3-a06f-4ee5-b988-28894e48067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, train_df.label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf68350f-ee02-4d47-b14f-0f33913d1427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1=0.8715245319534531\n",
      "prec=0.8574267005571962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#instantiate the model\n",
    "log_regression = LogisticRegression(penalty='l2')\n",
    "\n",
    "#fit the model using the training data\n",
    "log_regression.fit(X_train,y_train)\n",
    "\n",
    "#use model to make predictions on test data\n",
    "y_pred = log_regression.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test,y_pred, average='weighted')\n",
    "prec = precision_score(y_test,y_pred, average='weighted')\n",
    "print(f\"{f1=}\")\n",
    "print(f\"{prec=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea67143-b6c2-4643-8dc2-313f79f29bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6eb137-bbe7-4cd9-adc8-da0a22c7de57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0a2539d-f5d7-4395-a4d6-e7464c01ecd7",
   "metadata": {},
   "source": [
    "# Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "398d9e23-151d-4f43-896e-a2a75d8157f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "first_model_output = log_regression.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f94e960d-60c8-4889-a32d-8cf20c3fa61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1035904"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "806b8506-3e87-414b-bfa6-4dc7fa081e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['helper_1'] = first_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f771b2a-b0a4-4743-a123-530f52b638bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61: Arguement categories\n"
     ]
    }
   ],
   "source": [
    "def make_targets_categorical_label(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = df['target']\n",
    "    label_encoder.fit(y_train)\n",
    "    y_train = label_encoder.transform(y_train)\n",
    "    print(f'{len(set(y_train))}: Arguement categories')\n",
    "    return y_train\n",
    "y = make_targets_categorical_label(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76985a57-6767-4948-80e1-1f4cf400e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59c4433a-d5b5-4067-839d-7bed94d18f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#instantiate the model\n",
    "log_regression = LogisticRegression(penalty='l2')\n",
    "\n",
    "#fit the model using the training data\n",
    "log_regression.fit(X_train,y_train)\n",
    "\n",
    "#use model to make predictions on test data\n",
    "y_pred = log_regression.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6568f88-72b4-4c17-bf1a-ff87f7dc3591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1=0.8150204836393925\n",
      "prec=0.7636128367982268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test,y_pred, average='weighted')\n",
    "prec = precision_score(y_test,y_pred, average='weighted')\n",
    "print(f\"{f1=}\")\n",
    "print(f\"{prec=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7dc9d0-a8b9-400b-930c-1ed79e86d800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
