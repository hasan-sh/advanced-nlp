{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb48f5b1-dc74-4e1e-befa-429b46560e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea78999-1d7a-42e7-b184-2ad5d768643a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasan-sh/.virtualenvs/tm/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-24 21:09:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 11.8MB/s]\n",
      "2023-02-24 21:10:03 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-02-24 21:10:03 INFO: Use device: cpu\n",
      "2023-02-24 21:10:03 INFO: Loading: tokenize\n",
      "2023-02-24 21:10:03 INFO: Loading: pos\n",
      "2023-02-24 21:10:03 INFO: Loading: lemma\n",
      "2023-02-24 21:10:03 INFO: Loading: depparse\n",
      "2023-02-24 21:10:03 INFO: Loading: constituency\n",
      "2023-02-24 21:10:04 INFO: Loading: ner\n",
      "2023-02-24 21:10:04 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from features import extract_dependency_features, extract_features\n",
    "from utils import recombine_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b0a2b2f-ad71-46a0-a52a-1fafca618251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_file = '../data/en_ewt-up-train.conllu'\n",
    "test_file = '../data/en_ewt-up-test.conllu'\n",
    "\n",
    "def create_sentences(file_name):\n",
    "    # Open and read the input file\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Split the file into individual documents, each separated by a blank line\n",
    "    sentences = {}\n",
    "    for doc_i, doc in enumerate(content.split('\\n\\n')):\n",
    "        doc = doc.split('\\n')\n",
    "        sentence = []\n",
    "        for line in doc:\n",
    "            # Skip lines starting with '#' (comment lines)\n",
    "            if line and line[0] != '#':\n",
    "                line = line.split('\\t')\n",
    "                sentence.append(line[1])\n",
    "        sentences[doc_i] = ' '.join(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcad3a94-c8f3-4158-99c5-4cf4a64fa3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stanza\n",
    "# nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,ner')\n",
    "\n",
    "\n",
    "def get_named_entities(sentence):\n",
    "    \"\"\"\n",
    "    Get the named entity for each token in a data frame, and recombine the named entities into a list\n",
    "    of named entities for each sentence.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): A data frame with columns 'sent_id', 'token', and 'ner', where each row contains\n",
    "        a token and its named entity.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the sent_ids and the values are lists of named entities, where each named entity corresponds\n",
    "        to one token in the original sentence order.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use spaCy to extract named entities from the sentence\n",
    "    doc = nlp(sentence)\n",
    "    named_entities_for_sentence = [(ent.text, ent.type) for ent in doc.ents]\n",
    "\n",
    "    # Return the resulting dictionary of named entities\n",
    "    return named_entities_for_sentence\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(sentence, sent_id):\n",
    "    \"\"\"\n",
    "    Extracts various linguistic features from a given sentence using the Stanza library.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence to extract features from.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame of features extracted from the sentence, with the following columns:\n",
    "            - token (str): The original form of each token in the sentence.\n",
    "            - pos (str): The part-of-speech tag for each token.\n",
    "            - lemma (str): The lemma of each token.\n",
    "            - ner (str): The named entity label for each token (if any).\n",
    "            - stemming (str): The stemmed form of each token (using the Snowball stemmer).\n",
    "            - pos_bigram (str): A string representing the part-of-speech bigram for each token and its successor.\n",
    "            - token_bigram (str): A string representing the token bigram for each token and its successor.\n",
    "    \"\"\"\n",
    "    # Load the English model\n",
    "    # nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,ner')\n",
    "    \n",
    "    # Process the sentence\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Initialize lists to store the feature values\n",
    "    tokens = []\n",
    "    ner_tags = []\n",
    "    pos_bigrams = []\n",
    "    token_bigrams = []\n",
    "    \n",
    "    # Extract features for each token in the sentence\n",
    "    for i, sent in enumerate(doc.sentences):\n",
    "        ne = get_named_entities(sentences[sent_id])\n",
    "            \n",
    "        for j, word in enumerate(sent.words):\n",
    "            # Add token to list\n",
    "            tokens.append(word.text)\n",
    "            \n",
    "            # Add named entity label to list if it exists, otherwise add an empty string\n",
    "            named_entity = list(filter(lambda x: x[0] == word.text, ne))\n",
    "            if named_entity:\n",
    "                ner_tags.append(named_entity[0][1])\n",
    "            else:\n",
    "                ner_tags.append('_')\n",
    "            \n",
    "            \n",
    "            # Add part-of-speech bigram to list\n",
    "            if j < len(sent.words) - 1:\n",
    "                pos_bigrams.append(f\"{word.upos}_{sent.words[j+1].upos}\")\n",
    "            else:\n",
    "                pos_bigrams.append('')\n",
    "                \n",
    "            # Add token bigram to list\n",
    "            if j < len(sent.words) - 1:\n",
    "                token_bigrams.append(f\"{word.text}_{sent.words[j+1].text}\")\n",
    "            else:\n",
    "                token_bigrams.append('')\n",
    "    \n",
    "    return {\n",
    "        'token': tokens,\n",
    "        'ner': ner_tags,\n",
    "        'pos_bigram': pos_bigrams,\n",
    "        'token_bigram': token_bigrams\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3d4df17-e76c-4a02-9238-12160956617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = create_sentences('../data/en_ewt-up-train.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0c8d313-21ce-40fe-af5f-114289dd9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [extract_features(sent, i) for i, sent in sentences.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fb0197f-ee98-4c4d-bc93-85229a2edd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(result).explode(list(result[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81fe77f3-ff3e-4b3a-8699-7ae0f1fef718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4738/54734028.py:3: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train_1 = pd.read_csv(train_file, delimiter='\\t')\n"
     ]
    }
   ],
   "source": [
    "train_file = '../data/train.tsv'\n",
    "\n",
    "df_train_1 = pd.read_csv(train_file, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01e325c2-110c-4e5d-b87c-be974c560101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../data/train_ner.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6af2ff93-11e8-4e7e-9942-525d09eb036f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12543"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train_1.groupby('sent_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a3a1415-cc9a-4a4c-860a-ae11105f6f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test = create_sentences('../data/en_ewt-up-test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd7b2683-f894-4cf4-ab56-343520809b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = [extract_features(sent, i) for i, sent in sentences_test.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20d6ced4-5c7b-4fb1-96cd-f54e25141cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(result_test).explode(list(result[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb8fdd02-712b-4149-898a-ebfae76afdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4738/1300193031.py:3: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test_1 = pd.read_csv(test_file, delimiter='\\t')\n"
     ]
    }
   ],
   "source": [
    "test_file = '../data/test.tsv'\n",
    "\n",
    "df_test_1 = pd.read_csv(test_file, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d08be85b-052c-4594-a003-a1e1841fcaad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.to_csv('../data/test_ner.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c4ee45fc-14c4-4cfd-a449-77068355f800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>uni_POS</th>\n",
       "      <th>morph_type</th>\n",
       "      <th>distance_head</th>\n",
       "      <th>dep_label</th>\n",
       "      <th>dep_rel</th>\n",
       "      <th>space</th>\n",
       "      <th>probbank</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103236</th>\n",
       "      <td>1463</td>\n",
       "      <td>56.0</td>\n",
       "      <td>something</td>\n",
       "      <td>something</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NN</td>\n",
       "      <td>Number=Sing</td>\n",
       "      <td>53</td>\n",
       "      <td>obl</td>\n",
       "      <td>53:obl:to|58:nsubj|61:nsubj:xsubj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>ARG1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103237</th>\n",
       "      <td>1463</td>\n",
       "      <td>57.0</td>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>PRON</td>\n",
       "      <td>WDT</td>\n",
       "      <td>PronType=Rel</td>\n",
       "      <td>58</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>56:ref</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>R-ARG1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103238</th>\n",
       "      <td>1463</td>\n",
       "      <td>58.0</td>\n",
       "      <td>needs</td>\n",
       "      <td>need</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbF...</td>\n",
       "      <td>56</td>\n",
       "      <td>acl:relcl</td>\n",
       "      <td>56:acl:relcl</td>\n",
       "      <td>_</td>\n",
       "      <td>need.01</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103239</th>\n",
       "      <td>1463</td>\n",
       "      <td>59.0</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "      <td>TO</td>\n",
       "      <td>_</td>\n",
       "      <td>61</td>\n",
       "      <td>mark</td>\n",
       "      <td>61:mark</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103240</th>\n",
       "      <td>1463</td>\n",
       "      <td>60.0</td>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VB</td>\n",
       "      <td>VerbForm=Inf</td>\n",
       "      <td>61</td>\n",
       "      <td>aux:pass</td>\n",
       "      <td>61:aux:pass</td>\n",
       "      <td>_</td>\n",
       "      <td>be.03</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103241</th>\n",
       "      <td>1463</td>\n",
       "      <td>61.0</td>\n",
       "      <td>attended</td>\n",
       "      <td>attend</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>Tense=Past|VerbForm=Part|Voice=Pass</td>\n",
       "      <td>58</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>58:xcomp</td>\n",
       "      <td>_</td>\n",
       "      <td>attend.01</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103242</th>\n",
       "      <td>1463</td>\n",
       "      <td>62.0</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>_</td>\n",
       "      <td>61</td>\n",
       "      <td>obl</td>\n",
       "      <td>61:obl</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>C-ARG1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103243</th>\n",
       "      <td>1463</td>\n",
       "      <td>63.0</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>right</td>\n",
       "      <td>ADV</td>\n",
       "      <td>RB</td>\n",
       "      <td>_</td>\n",
       "      <td>64</td>\n",
       "      <td>advmod</td>\n",
       "      <td>64:advmod</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103244</th>\n",
       "      <td>1463</td>\n",
       "      <td>64.0</td>\n",
       "      <td>AWAY</td>\n",
       "      <td>away</td>\n",
       "      <td>ADV</td>\n",
       "      <td>RB</td>\n",
       "      <td>_</td>\n",
       "      <td>61</td>\n",
       "      <td>advmod</td>\n",
       "      <td>61:advmod</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "      <td>_</td>\n",
       "      <td>ARGM-TMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103245</th>\n",
       "      <td>1463</td>\n",
       "      <td>65.0</td>\n",
       "      <td>!!!</td>\n",
       "      <td>!!!</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>_</td>\n",
       "      <td>24</td>\n",
       "      <td>punct</td>\n",
       "      <td>24:punct</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sent_id  token_id      token      lemma    POS uni_POS  \\\n",
       "103236     1463      56.0  something  something   PRON      NN   \n",
       "103237     1463      57.0       that       that   PRON     WDT   \n",
       "103238     1463      58.0      needs       need   VERB     VBZ   \n",
       "103239     1463      59.0         to         to   PART      TO   \n",
       "103240     1463      60.0         be         be    AUX      VB   \n",
       "103241     1463      61.0   attended     attend   VERB     VBN   \n",
       "103242     1463      62.0         to         to    ADP      IN   \n",
       "103243     1463      63.0      RIGHT      right    ADV      RB   \n",
       "103244     1463      64.0       AWAY       away    ADV      RB   \n",
       "103245     1463      65.0        !!!        !!!  PUNCT       .   \n",
       "\n",
       "                                               morph_type distance_head  \\\n",
       "103236                                        Number=Sing            53   \n",
       "103237                                       PronType=Rel            58   \n",
       "103238  Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbF...            56   \n",
       "103239                                                  _            61   \n",
       "103240                                       VerbForm=Inf            61   \n",
       "103241                Tense=Past|VerbForm=Part|Voice=Pass            58   \n",
       "103242                                                  _            61   \n",
       "103243                                                  _            64   \n",
       "103244                                                  _            61   \n",
       "103245                                                  _            24   \n",
       "\n",
       "        dep_label                            dep_rel          space  \\\n",
       "103236        obl  53:obl:to|58:nsubj|61:nsubj:xsubj              _   \n",
       "103237      nsubj                             56:ref              _   \n",
       "103238  acl:relcl                       56:acl:relcl              _   \n",
       "103239       mark                            61:mark              _   \n",
       "103240   aux:pass                        61:aux:pass              _   \n",
       "103241      xcomp                           58:xcomp              _   \n",
       "103242        obl                             61:obl              _   \n",
       "103243     advmod                          64:advmod              _   \n",
       "103244     advmod                          61:advmod  SpaceAfter=No   \n",
       "103245      punct                           24:punct              _   \n",
       "\n",
       "         probbank    target  \n",
       "103236          _      ARG1  \n",
       "103237          _    R-ARG1  \n",
       "103238    need.01         _  \n",
       "103239          _         _  \n",
       "103240      be.03         _  \n",
       "103241  attend.01         V  \n",
       "103242          _    C-ARG1  \n",
       "103243          _         _  \n",
       "103244          _  ARGM-TMP  \n",
       "103245          _         _  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_1.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7366cca-7478-45cb-aa0d-8e4606998ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0c8cdfd5-05bc-4ce5-8e35-8e484e0c3804",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m nlp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHow does it work?\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(t\u001b[38;5;241m.\u001b[39mner)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Document' object is not iterable"
     ]
    }
   ],
   "source": [
    "for t in nlp('How does it work?'):\n",
    "    print(t.ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef38b92-14af-409e-aabc-382c16d77800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_fe = pd.concat([basic_fe, dependency_fe], axis=1)\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "for i, s in sentences.items():\n",
    "    basic_fe = extract_features(s)\n",
    "    dependency_fe = extract_dependency_features(s)\n",
    "    \n",
    "    df = pd.concat([df, pd.concat([basic_fe, dependency_fe], axis=1)])\n",
    "    print(basic_fe)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36cb06-67e1-4d9c-9ba0-23bdf969d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "0881122112 # gza health care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee54492-4f7d-4fd1-bf09-0d35e27931fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
