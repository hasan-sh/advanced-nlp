{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cbab12f-d279-4b4e-af94-637cce6ff0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 13:41:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f813d440d1b444498deeebbdb0d14e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 13:41:27 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-02-25 13:41:27 INFO: Use device: gpu\n",
      "2023-02-25 13:41:27 INFO: Loading: tokenize\n",
      "2023-02-25 13:41:29 INFO: Loading: pos\n",
      "2023-02-25 13:41:30 INFO: Loading: lemma\n",
      "2023-02-25 13:41:30 INFO: Loading: ner\n",
      "2023-02-25 13:41:30 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# !pip install stanza\n",
    "# Load the English model\n",
    "import stanza\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,ner')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4139308a-d774-4d55-adda-75aca9cd5713",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/en_ewt-up-train.conllu'\n",
    "test_file = '../data/en_ewt-up-test.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033090b4-bfd0-43af-9e68-17652a5dec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_entities(sentence):\n",
    "    \"\"\"\n",
    "    Get the named entity for each token in a data frame, and recombine the named entities into a list\n",
    "    of named entities for each sentence.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): A data frame with columns 'sent_id', 'token', and 'ner', where each row contains\n",
    "        a token and its named entity.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the sent_ids and the values are lists of named entities, where each named entity corresponds\n",
    "        to one token in the original sentence order.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use spaCy to extract named entities from the sentence\n",
    "    doc = nlp(sentence)\n",
    "    named_entities_for_sentence = [(ent.text, ent.type) for ent in doc.ents]\n",
    "    \n",
    "    # Return the resulting dictionary of named entities\n",
    "    return named_entities_for_sentence\n",
    "\n",
    "def assign_ne(row, named_enities):\n",
    "    named_entity = list(filter(lambda x: row.token in x[0], named_enities))\n",
    "    if named_entity:\n",
    "        return named_entity[0][1]\n",
    "    else:\n",
    "        return '_'\n",
    "\n",
    "def add_bigrams(df):\n",
    "    \"\"\"\n",
    "    Adds columns for token and pos bigrams to a DataFrame containing token and pos columns.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): A DataFrame containing \"token\" and \"pos\" columns.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame with additional columns for token and pos bigrams.\n",
    "    \"\"\"\n",
    "    # Create token bigrams\n",
    "    df[\"token_bigram\"] = pd.Series(list(zip(df[\"token\"].shift(), df[\"token\"])))\n",
    "    # Create pos bigrams\n",
    "    df[\"pos_bigram\"] = pd.Series(list(zip(df[\"POS\"].shift(), df[\"POS\"])))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11c19708-f841-4b42-8ba3-2bd7a733ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path, save_to_csv=False):\n",
    "    \"\"\"\n",
    "    This function reads a CoNLL-U format file and converts it into a pandas DataFrame.\n",
    "    Each row in the DataFrame corresponds to a token in the file, and columns\n",
    "    correspond to different features of the token, such as the token itself, its lemma, \n",
    "    part-of-speech tag, and syntactic dependency information.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the input CoNLL-U format file.\n",
    "    save_to_csv (bool): A boolean flag indicating whether to save the resulting DataFrame \n",
    "                        to a CSV file. Default is False.\n",
    "                        \n",
    "    Returns:\n",
    "    df (pandas.DataFrame): A pandas DataFrame containing the token-level information from\n",
    "                           the input file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open and read the input file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        train_data = f.read()\n",
    "    \n",
    "    # Split the file into individual documents, each separated by a blank line\n",
    "    data = []\n",
    "    for doc_i, doc in enumerate(train_data.split('\\n\\n')):\n",
    "        doc = doc.split('\\n')\n",
    "        sentences = ''\n",
    "        for line in doc:\n",
    "            # Skip lines starting with '#' (comment lines)\n",
    "            if line and line[0] != '#':\n",
    "                line = line.split('\\t')\n",
    "                line.insert(0, str(doc_i))\n",
    "                sentences += '\\t'.join(line) + '\\n'\n",
    "        data.append(sentences)\n",
    "    \n",
    "    # Create a pandas DataFrame from the token-level data\n",
    "    train_df = pd.DataFrame([x.split('\\t') for sent in data for x in sent.split('\\n') if x])\n",
    "    \n",
    "    # Rename the columns of the DataFrame\n",
    "    train_df = train_df.rename(columns={\n",
    "        0:'sent_id', \n",
    "        1:'token_id', \n",
    "        2:'token', \n",
    "        3:'lemma', \n",
    "        4:'POS', \n",
    "        5:'uni_POS',\n",
    "        6:'morph_type', \n",
    "        7:'distance_head', \n",
    "        8:'dep_label', \n",
    "        9:'dep_rel', \n",
    "        10:'space', \n",
    "        11:'probbank'\n",
    "    })\n",
    "    \n",
    "    named_enities_dict = {}\n",
    "    for sent_id, group in train_df.groupby('sent_id'):\n",
    "        sentence_tokens = list(group['token'])\n",
    "        sentence = ' '.join(sentence_tokens)\n",
    "        ne = get_named_entities(sentence)\n",
    "        named_enities_dict[sent_id] = ne\n",
    "    \n",
    "                \n",
    "    train_df['ner'] = train_df.apply(lambda x: assign_ne(x, named_enities_dict[x['sent_id']]), axis=1)\n",
    "    train_df = add_bigrams(train_df)  \n",
    "    # Convert the DataFrame from wide to long format\n",
    "    df = train_df.melt(\n",
    "        id_vars=[i for i in train_df.columns[:12]]+['ner','token_bigram','pos_bigram'], \n",
    "        var_name=\"notneeded\", \n",
    "        value_name=\"target\"\n",
    "    )\n",
    "    \n",
    "    # Drop the 'notneeded' column and any rows that contain missing values\n",
    "    df.drop(['notneeded'], axis=1, inplace=True)\n",
    "    df = df[df['target'].notna()]\n",
    "    \n",
    "    # Optionally save the resulting DataFrame to a CSV file\n",
    "    if save_to_csv:\n",
    "        df.to_csv('../data/train_ner.tsv', sep='\\t', index=False)\n",
    "        # df.to_csv('../data/test_ner.tsv', sep='\\t', index=False)\n",
    "    \n",
    "    # Return the resulting DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5d4a042-df70-46a3-9c3e-0fe29f747d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = read_data(train_file, save_to_csv=True)\n",
    "# x = read_data(test_file, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82ba9cd6-90ba-4647-95c4-1db4e9336526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>uni_POS</th>\n",
       "      <th>morph_type</th>\n",
       "      <th>distance_head</th>\n",
       "      <th>dep_label</th>\n",
       "      <th>dep_rel</th>\n",
       "      <th>space</th>\n",
       "      <th>probbank</th>\n",
       "      <th>ner</th>\n",
       "      <th>token_bigram</th>\n",
       "      <th>pos_bigram</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Al</td>\n",
       "      <td>Al</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Number=Sing</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>0:root</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "      <td>_</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>(None, Al)</td>\n",
       "      <td>(None, PROPN)</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>HYPH</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>punct</td>\n",
       "      <td>1:punct</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "      <td>_</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>(Al, -)</td>\n",
       "      <td>(PROPN, PUNCT)</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Zaman</td>\n",
       "      <td>Zaman</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Number=Sing</td>\n",
       "      <td>1</td>\n",
       "      <td>flat</td>\n",
       "      <td>1:flat</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>(-, Zaman)</td>\n",
       "      <td>(PUNCT, PROPN)</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>:</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>punct</td>\n",
       "      <td>1:punct</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>(Zaman, :)</td>\n",
       "      <td>(PROPN, PUNCT)</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>American</td>\n",
       "      <td>american</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Degree=Pos</td>\n",
       "      <td>6</td>\n",
       "      <td>amod</td>\n",
       "      <td>6:amod</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>NORP</td>\n",
       "      <td>(:, American)</td>\n",
       "      <td>(PUNCT, ADJ)</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7077906</th>\n",
       "      <td>7506</td>\n",
       "      <td>131</td>\n",
       "      <td>graduated</td>\n",
       "      <td>graduate</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>Tense=Past|VerbForm=Part</td>\n",
       "      <td>123</td>\n",
       "      <td>advcl</td>\n",
       "      <td>123:advcl:since</td>\n",
       "      <td>_</td>\n",
       "      <td>graduate.01</td>\n",
       "      <td>_</td>\n",
       "      <td>(have, graduated)</td>\n",
       "      <td>(AUX, VERB)</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7077907</th>\n",
       "      <td>7506</td>\n",
       "      <td>132</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>_</td>\n",
       "      <td>134</td>\n",
       "      <td>cc</td>\n",
       "      <td>134:cc</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>(graduated, and)</td>\n",
       "      <td>(VERB, CCONJ)</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7077908</th>\n",
       "      <td>7506</td>\n",
       "      <td>133</td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Case=Nom|Number=Sing|Person=1|PronType=Prs</td>\n",
       "      <td>134</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>134:nsubj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>(and, i)</td>\n",
       "      <td>(CCONJ, PRON)</td>\n",
       "      <td>ARG0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7077909</th>\n",
       "      <td>7506</td>\n",
       "      <td>134</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBP</td>\n",
       "      <td>Mood=Ind|Tense=Pres|VerbForm=Fin</td>\n",
       "      <td>4</td>\n",
       "      <td>conj</td>\n",
       "      <td>4:conj:and</td>\n",
       "      <td>_</td>\n",
       "      <td>hate.01</td>\n",
       "      <td>_</td>\n",
       "      <td>(i, hate)</td>\n",
       "      <td>(PRON, VERB)</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7077910</th>\n",
       "      <td>7506</td>\n",
       "      <td>135</td>\n",
       "      <td>drama</td>\n",
       "      <td>drama</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>Number=Sing</td>\n",
       "      <td>134</td>\n",
       "      <td>obj</td>\n",
       "      <td>134:obj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>(hate, drama)</td>\n",
       "      <td>(VERB, NOUN)</td>\n",
       "      <td>ARG1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1035928 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sent_id token_id      token     lemma    POS uni_POS  \\\n",
       "0             0        1         Al        Al  PROPN     NNP   \n",
       "1             0        2          -         -  PUNCT    HYPH   \n",
       "2             0        3      Zaman     Zaman  PROPN     NNP   \n",
       "3             0        4          :         :  PUNCT       :   \n",
       "4             0        5   American  american    ADJ      JJ   \n",
       "...         ...      ...        ...       ...    ...     ...   \n",
       "7077906    7506      131  graduated  graduate   VERB     VBN   \n",
       "7077907    7506      132        and       and  CCONJ      CC   \n",
       "7077908    7506      133          i         i   PRON     PRP   \n",
       "7077909    7506      134       hate      hate   VERB     VBP   \n",
       "7077910    7506      135      drama     drama   NOUN      NN   \n",
       "\n",
       "                                         morph_type distance_head dep_label  \\\n",
       "0                                       Number=Sing             0      root   \n",
       "1                                                 _             1     punct   \n",
       "2                                       Number=Sing             1      flat   \n",
       "3                                                 _             1     punct   \n",
       "4                                        Degree=Pos             6      amod   \n",
       "...                                             ...           ...       ...   \n",
       "7077906                    Tense=Past|VerbForm=Part           123     advcl   \n",
       "7077907                                           _           134        cc   \n",
       "7077908  Case=Nom|Number=Sing|Person=1|PronType=Prs           134     nsubj   \n",
       "7077909            Mood=Ind|Tense=Pres|VerbForm=Fin             4      conj   \n",
       "7077910                                 Number=Sing           134       obj   \n",
       "\n",
       "                 dep_rel          space     probbank     ner  \\\n",
       "0                 0:root  SpaceAfter=No            _  PERSON   \n",
       "1                1:punct  SpaceAfter=No            _  PERSON   \n",
       "2                 1:flat              _            _  PERSON   \n",
       "3                1:punct              _            _       _   \n",
       "4                 6:amod              _            _    NORP   \n",
       "...                  ...            ...          ...     ...   \n",
       "7077906  123:advcl:since              _  graduate.01       _   \n",
       "7077907           134:cc              _            _       _   \n",
       "7077908        134:nsubj              _            _  PERSON   \n",
       "7077909       4:conj:and              _      hate.01       _   \n",
       "7077910          134:obj              _            _       _   \n",
       "\n",
       "              token_bigram      pos_bigram target  \n",
       "0               (None, Al)   (None, PROPN)      _  \n",
       "1                  (Al, -)  (PROPN, PUNCT)      _  \n",
       "2               (-, Zaman)  (PUNCT, PROPN)      _  \n",
       "3               (Zaman, :)  (PROPN, PUNCT)      _  \n",
       "4            (:, American)    (PUNCT, ADJ)      _  \n",
       "...                    ...             ...    ...  \n",
       "7077906  (have, graduated)     (AUX, VERB)      _  \n",
       "7077907   (graduated, and)   (VERB, CCONJ)      _  \n",
       "7077908           (and, i)   (CCONJ, PRON)   ARG0  \n",
       "7077909          (i, hate)    (PRON, VERB)      V  \n",
       "7077910      (hate, drama)    (VERB, NOUN)   ARG1  \n",
       "\n",
       "[1035928 rows x 16 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2caa01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', 50)\n",
    "# pd.set_option('display.min_rows', 200)\n",
    "# pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18714eb5-2060-47b5-af4c-3a83afc234ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
