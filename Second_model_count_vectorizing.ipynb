{"cells":[{"cell_type":"code","execution_count":null,"id":"7cbab12f-d279-4b4e-af94-637cce6ff0cb","metadata":{"id":"7cbab12f-d279-4b4e-af94-637cce6ff0cb"},"outputs":[],"source":[]},{"cell_type":"code","source":["!git clone https://github.com/hasan-sh/advanced-nlp.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WScyFUAFDrGl","outputId":"f2b37980-3cc4-4b7f-99c0-45a270703ddb","executionInfo":{"status":"ok","timestamp":1677330477744,"user_tz":-60,"elapsed":7795,"user":{"displayName":"Γιώργος Φραδέλος","userId":"06098843364769382863"}}},"id":"WScyFUAFDrGl","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'advanced-nlp'...\n","remote: Enumerating objects: 120, done.\u001b[K\n","remote: Counting objects: 100% (120/120), done.\u001b[K\n","remote: Compressing objects: 100% (95/95), done.\u001b[K\n","remote: Total 120 (delta 51), reused 63 (delta 15), pack-reused 0\u001b[K\n","Receiving objects: 100% (120/120), 18.67 MiB | 6.68 MiB/s, done.\n","Resolving deltas: 100% (51/51), done.\n"]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4I0iZzGdD2i6","outputId":"c9a7bb47-413b-412e-a4d9-67b52a18fa7b","executionInfo":{"status":"ok","timestamp":1677330477746,"user_tz":-60,"elapsed":17,"user":{"displayName":"Γιώργος Φραδέλος","userId":"06098843364769382863"}}},"id":"4I0iZzGdD2i6","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n","\n","from sklearn.metrics import *\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n"],"metadata":{"id":"EuJPshDtoHqb","executionInfo":{"status":"ok","timestamp":1677330487938,"user_tz":-60,"elapsed":3612,"user":{"displayName":"Γιώργος Φραδέλος","userId":"06098843364769382863"}}},"id":"EuJPshDtoHqb","execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"id":"4139308a-d774-4d55-adda-75aca9cd5713","metadata":{"id":"4139308a-d774-4d55-adda-75aca9cd5713","executionInfo":{"status":"ok","timestamp":1677330490317,"user_tz":-60,"elapsed":9,"user":{"displayName":"Γιώργος Φραδέλος","userId":"06098843364769382863"}}},"outputs":[],"source":["train_file = '/content/drive/MyDrive/VU/Advanced NLP/Ass. 2/output_firstmodel_baseline.csv'\n","test_file = '/content/advanced-nlp/data/en_ewt-up-test.conllu'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrf8ah0XzS8F"},"outputs":[],"source":["def read_data(file_path, save_to_csv=False):\n","    \"\"\"\n","    This function reads a CoNLL-U format file and converts it into a pandas DataFrame.\n","    Each row in the DataFrame corresponds to a token in the file, and columns\n","    correspond to different features of the token, such as the token itself, its lemma, \n","    part-of-speech tag, and syntactic dependency information.\n","    \n","    Parameters:\n","    file_path (str): The path to the input CoNLL-U format file.\n","    save_to_csv (bool): A boolean flag indicating whether to save the resulting DataFrame \n","                        to a CSV file. Default is False.\n","                        \n","    Returns:\n","    df (pandas.DataFrame): A pandas DataFrame containing the token-level information from\n","                           the input file.\n","    \"\"\"\n","    \n","    # Open and read the input file\n","    with open(file_path, 'r', encoding='utf-8', ) as f:\n","        train_data = f.read()\n","    \n","    # Split the file into individual documents, each separated by a blank line\n","    data = []\n","    for doc_i, doc in enumerate(train_data.split('\\n\\n')):\n","        doc = doc.split('\\n')\n","        sentences = ''\n","        for line in doc:\n","            # Skip lines starting with '#' (comment lines)\n","            if line and line[0] != '#':\n","                line = line.split('\\t')\n","                line.insert(0, str(doc_i))\n","                sentences += '\\t'.join(line) + '\\n'\n","        data.append(sentences)\n","    \n","    # Create a pandas DataFrame from the token-level data\n","    train_df = pd.DataFrame([x.split('\\t') for sent in data for x in sent.split('\\n') if x])\n","    \n","    # Rename the columns of the DataFrame\n","    train_df = train_df.rename(columns={\n","        0:'sent_id', \n","        1:'token_id', \n","        2:'token', \n","        3:'lemma', \n","        4:'POS', \n","        5:'uni_POS',\n","        6:'morph_type', \n","        7:'distance_head', \n","        8:'dep_label', \n","        9:'dep_rel', \n","        10:'space', \n","        11:'probbank'\n","    })\n","    \n","    # Convert the DataFrame from wide to long format\n","    df = train_df.melt(\n","        id_vars=[i for i in train_df.columns[:12]], \n","        var_name=\"notneeded\", \n","        value_name=\"target\"\n","    )\n","    \n","    # Drop the 'notneeded' column and any rows that contain missing values\n","    #df[\"sent_id\"]=df['sent_id'].str.cat((df['notneeded'].astype(int)-12).astype(str) , sep=\"_\" )\n","    df[\"repetion_id\"]=df[\"notneeded\"]-12\n","    df.drop(['notneeded'], axis=1, inplace=True)\n","    df = df[df['target'].notna()]\n","    \n","    # Optionally save the resulting DataFrame to a CSV file\n","    if save_to_csv:\n","        df.to_csv('/content/advanced-nlp/data/test.tsv', sep='\\t', index=False)\n","    \n","    # Return the resulting DataFrame\n","\n","    return df"],"id":"mrf8ah0XzS8F"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"3XzIllt1cU7e","executionInfo":{"status":"ok","timestamp":1677279971098,"user_tz":-60,"elapsed":19570,"user":{"displayName":"Γιώργος Φραδέλος","userId":"06098843364769382863"}},"outputId":"0541e4c9-c040-4410-bb58-3f46b67f4eb4","colab":{"base_uri":"https://localhost:8080/"}},"id":"3XzIllt1cU7e","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"id":"d5d4a042-df70-46a3-9c3e-0fe29f747d4b","metadata":{"id":"d5d4a042-df70-46a3-9c3e-0fe29f747d4b"},"outputs":[],"source":["#train = read_data(train_file, save_to_csv=True)\n","\n","# Open and read the output of the 1st model\n","train = pd.read_csv(train_file)\n","\n","# remove rows with label zero\n","train = train[train.predictions != 0]\n","\n","test = read_data(test_file, save_to_csv=True)"]},{"cell_type":"code","source":["\n","## Ignore those functions (they are also the same at the +features)\n","def make_targets_categorical_label(df):\n","\n","  ## Create an instance of CountVectorizer for training data tokens\n","  #count_vectorizer = CountVectorizer()\n","\n","  # Create an instance of LabelEncoder for training data targets\n","  label_encoder = LabelEncoder()\n","\n","\n","\n","\n","  #x_train = df['token']\n","  y_train = df['target']\n","\n"," \n","  # Fit the label encoder to the tokens\n","  label_encoder.fit(y_train)\n","\n","  #y_all = np.concatenate([y_train], axis=0)\n","\n","  # Fit the label encoder to the targets\n","  label_encoder.fit(y_train)\n","\n","\n","  # Encode training data y\n","  y_train = label_encoder.transform(y_train)\n","\n","\n","  #print(X_train)\n","  print(f'{len(set(y_train))}: Arguement categories')\n","\n","\n","  #df = df.assign(label=[0 if target==\"_\" or target==\"V\" else 1 for target in df['target']])\n","  #df= df.drop('target', axis=1)\n","  return y_train\n","\n","#clean column\n","def columns_cleaning(df): #political choices inside\n","  \"\"\"This function perform a preprocessing steps that consists of:\n","  - in removing rows with missinvg value for distance head feature\n","  - cast numerical features to int \n","  - drop token_id (I belive is useless)\n","  \n","\n","  \"\"\"\n","  df=df[df[\"distance_head\"]!=\"_\"]#like this\n","  df[\"distance_head\"]=df[\"distance_head\"].astype(int) #WARNING IS FROM HERE\n","  \n","  df[\"sent_id\"]=df[\"sent_id\"].astype(int) #WARNING IS FROM HERE\n","\n","  df= df.drop('token_id', axis=1) #or this\n","  df=df[['sent_id', 'repetion_id','token', 'lemma', 'POS', 'uni_POS', 'morph_type',\n","        'distance_head', 'dep_label', 'dep_rel', 'space', 'probbank' ,'label']]\n","\n","  return df\n","\n","\n","def make_NER(df):\n","  return df\n","\n","\n","\"\"\"cols_to_encode=[ 'token', 'lemma', 'POS', 'uni_POS',\n","       'morph_type', 'dep_label', 'dep_rel', 'space',\n","       'probbank']\"\"\"\n","\n","\"\"\"cols_to_encode=[ 'POS', 'uni_POS',\n","       'morph_type', 'dep_label', 'dep_rel', 'space',\n","       'probbank']\"\"\"\n","\n","\n","def create_encoding(train_df,test_df,cols_to_encode):\n","  \"\"\"This function creates a label encoding (just assign number to every value) for all columns in the list cols_to_encode \"\"\"\n","  # create a LabelEncoder objec\n","  oe = OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1)\n","  train_df=oe.fit_transform(df_train[cols_to_encode])\n","  test_df = oe.transform(df_test[cols_to_encode])\n","  #print(oe.categories_)\n","  # iterate over the columns to encode\n","  \"\"\"for col in cols_to_encode:\n","      train_df[col] = oe.fit_transform(train_df[col])\n","      test_df[col] = oe.transform(test_df[col])\n","  \"\"\"\n","  return train_df, test_df\n"],"metadata":{"id":"T_1k9SPRri2Y"},"id":"T_1k9SPRri2Y","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##BASELINE WITH ONLY TOKEN"],"metadata":{"id":"fdE6rrG-mgRD"},"id":"fdE6rrG-mgRD"},{"cell_type":"code","source":["def prepare_token_data(tokens_train, tokens_test):\n","\n","  encoder = LabelEncoder()\n","  vectorizer = CountVectorizer()\n","\n","  X_train = vectorizer.fit_transform(tokens_train['token'])\n","  X_test = vectorizer.transform(tokens_test['token'])\n","  \n","\n","  y_train = tokens_train['target']\n","  y_test = tokens_test['target']\n","  y_all = np.concatenate([y_train, y_test], axis=0)\n","\n","\n","  # Fit the label encoder to the targets\n","  encoder.fit(y_all.astype(str))\n","\n","  \n","  y_train = encoder.transform(y_train.astype(str))\n","  y_test = encoder.transform(y_test.astype(str))\n","\n","\n","\n","\n","  return X_train, X_test, y_train, y_test, encoder\n"],"metadata":{"id":"wiWehoZrEiV5"},"execution_count":null,"outputs":[],"id":"wiWehoZrEiV5"},{"cell_type":"code","source":["def logistic_reg(X_train, X_test, y_train, y_test):\n","  #instantiate the model\n","  log_regression = LogisticRegression(penalty='l2')\n","\n","  #fit the model using the training data\n","  log_regression.fit(X_train, y_train)\n","\n","  #use model to make predictions on test data\n","  y_pred = log_regression.predict(X_test)\n","  return y_pred"],"metadata":{"id":"-zNTlLupFcnm"},"id":"-zNTlLupFcnm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def confusion_mtrx(y_test, y_pred):\n","\n","  # get all categories of LabelEncoder in list\n","  categories = list(set(y_test))\n","\n","  size = len(categories)\n","\n","  cm = confusion_matrix(y_test, y_pred)\n","  ax= plt.subplot()\n","\n","  #ax.rcParams['figure.figsize'] = [3.8,3.8]\n","\n","  sns.heatmap(cm, annot=True, fmt='g', ax=ax,cmap='Blues');  #annot=True to annotate cells, ftm='g' to disable scientific notation\n","  sns.color_palette(\"tab10\")  \n","  \n","\n","\n","  # labels, title and ticks\n","  ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n","  ax.set_title('Confusion Matrix'); \n","  ax.xaxis.set_ticklabels(categories); ax.yaxis.set_ticklabels(categories);\n","  return 0"],"metadata":{"id":"IlP1AbJ9cZQZ"},"id":"IlP1AbJ9cZQZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#print(train)\n","\n","tokens_train = train[[\"token\", \"target\"]]\n","tokens_test = test[[\"token\", \"target\"]]\n","\n","\n","X_train, X_test, y_train, y_test, encoder = prepare_token_data(tokens_train, tokens_test)\n","\n","# reshape data so they fit to model\n","#X_train = X_train.reshape(-1, 1)\n","#X_test = X_test.reshape(-1, 1)\n","\n","print(X_train.shape)\n","print(y_train.shape)\n","\n","\n","print(f'\\n{X_test.shape}')\n","print(y_test.shape)\n","\n","y_pred = logistic_reg(X_train, X_test, y_train, y_test)\n","f1 = f1_score(y_test, y_pred, average='micro')\n","print(f\"\\n{f1=}\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","\n","\n","# Decode the predicted target values\n","y_test = encoder.inverse_transform(y_test)\n","\n","# Decode the actual target values\n","y_pred = encoder.inverse_transform(y_pred)\n"],"metadata":{"id":"4PZnvYkrn43V","colab":{"base_uri":"https://localhost:8080/"},"outputId":"54bc4f9a-0a0f-4fe0-fbc5-8287604ba6b3","executionInfo":{"status":"ok","timestamp":1677280070166,"user_tz":-60,"elapsed":42373,"user":{"displayName":"Γιώργος Φραδέλος","userId":"06098843364769382863"}}},"id":"4PZnvYkrn43V","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(83607, 4178)\n","(83607,)\n","\n","(103246, 4178)\n","(103246,)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["\n","f1=0.7494043352769115\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00      2102\n","           1       0.17      0.29      0.21      1733\n","           2       0.13      0.27      0.18      3241\n","           3       0.00      0.00      0.00         4\n","           4       0.11      0.09      0.10      1129\n","           5       0.00      0.00      0.00        74\n","           6       0.00      0.00      0.00        56\n","           7       0.00      0.00      0.00         1\n","           8       0.00      0.00      0.00         2\n","           9       0.00      0.00      0.00       228\n","          10       0.19      0.16      0.17       496\n","          11       0.20      0.20      0.20        46\n","          12       0.00      0.00      0.00        13\n","          13       0.00      0.00      0.00        12\n","          14       0.00      0.00      0.00        47\n","          15       0.24      0.40      0.30       182\n","          16       0.20      0.13      0.16       105\n","          17       0.00      0.00      0.00        24\n","          18       0.11      0.02      0.03       207\n","          19       0.00      0.00      0.00        69\n","          20       0.10      0.08      0.09       148\n","          21       0.24      0.96      0.39       442\n","          22       0.19      0.52      0.28       216\n","          23       0.00      0.00      0.00        44\n","          24       0.17      0.03      0.05        75\n","          25       0.00      0.00      0.00        69\n","          27       0.18      0.52      0.26       543\n","          28       0.00      0.00      0.00         3\n","          29       0.00      0.00      0.00        52\n","          30       0.00      0.00      0.00         1\n","          31       0.00      0.00      0.00         7\n","          32       0.00      0.00      0.00         2\n","          34       0.00      0.00      0.00         5\n","          35       0.00      0.00      0.00         1\n","          38       0.00      0.00      0.00        16\n","          39       0.15      0.40      0.22        67\n","          40       0.00      0.00      0.00        52\n","          41       0.00      0.00      0.00         1\n","          44       0.00      0.00      0.00         1\n","          45       0.00      0.00      0.00         1\n","          47       0.00      0.00      0.00         1\n","          49       0.00      0.00      0.00         9\n","          50       0.00      0.00      0.00         8\n","          51       0.00      0.00      0.00         2\n","          52       0.00      0.00      0.00      4802\n","          53       0.86      0.86      0.86     86907\n","          54       0.00      0.00      0.00         0\n","\n","    accuracy                           0.75    103246\n","   macro avg       0.07      0.10      0.07    103246\n","weighted avg       0.74      0.75      0.74    103246\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["#categories = list(set(y_test))\n","print((list(set(y_test))))\n","print(len(list(set(y_pred))))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDZIDx9m_nP4","executionInfo":{"status":"ok","timestamp":1677271908181,"user_tz":-60,"elapsed":237,"user":{"displayName":"Γιώργος Φραδέλος","userId":"06098843364769382863"}},"outputId":"96cc4291-878c-4baf-9e5c-0407a71649ff"},"id":"vDZIDx9m_nP4","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['', 'ARGM-LOC', 'ARG3', 'ARGM-LVB', 'ARG2', 'ARGM-DIS', 'R-ARG0', 'C-ARG1-DSP', 'ARGM-DIR', 'R-ARGM-LOC', 'R-ARGM-MNR', 'ARGM-PRR', 'ARGM-NEG', 'C-ARGM-LOC', 'ARGM-ADV', 'ARGM-PRD', 'ARGM-MNR', 'R-ARGM-ADV', 'ARGM-ADJ', 'ARG0', 'ARG5', 'C-ARGM-CXN', 'C-ARG3', 'V', 'ARG4', 'C-V', 'ARGM-TMP', 'ARGM-COM', 'ARG1', 'ARGM-EXT', 'R-ARGM-TMP', 'ARGA', 'ARG1-DSP', 'ARGM-MOD', 'R-ARG1', 'C-ARG0', 'R-ARGM-ADJ', 'R-ARG2', 'ARGM-GOL', 'C-ARG1', '_', 'C-ARG2', 'ARGM-CAU', 'R-ARGM-DIR', 'ARGM-PRP', 'ARGM-CXN']\n","16\n"]}]},{"cell_type":"code","source":["confusion_mtrx(y_test, y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":551},"id":"q4aAhhootGs0","outputId":"c6dda5c3-0dac-4777-9cb2-a43332704f13","executionInfo":{"status":"error","timestamp":1677272254367,"user_tz":-60,"elapsed":577,"user":{"displayName":"Γιώργος Φραδέλος","userId":"06098843364769382863"}}},"id":"q4aAhhootGs0","execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-173-46eb16a11d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfusion_mtrx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-172-0f3d9eb320f0>\u001b[0m in \u001b[0;36mconfusion_mtrx\u001b[0;34m(y_test, y_pred)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3.8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Blues'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0;31m#annot=True to annotate cells, ftm='g' to disable scientific notation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'rcParams'"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":[],"metadata":{"id":"UEWnGCyJyFsc"},"id":"UEWnGCyJyFsc","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yXmmC6uzfjl1"},"id":"yXmmC6uzfjl1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# WITH BASIC FEATURES"],"metadata":{"id":"HoGHIdlTnosD"},"id":"HoGHIdlTnosD"},{"cell_type":"code","source":["# To do\n","# \n"],"metadata":{"id":"HeRMhzyeBdjh"},"id":"HeRMhzyeBdjh","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Include additional features to train and test data (Pipeline)\n","\n"],"metadata":{"id":"spWC0-lEOte0"},"id":"spWC0-lEOte0"},{"cell_type":"code","source":["def make_targets_categorical_label(df):\n","\n","  ## Create an instance of CountVectorizer for training data tokens\n","  #count_vectorizer = CountVectorizer()\n","\n","  # Create an instance of LabelEncoder for training data targets\n","  label_encoder = LabelEncoder()\n","\n","\n","\n","\n","  #x_train = df['token']\n","  y_train = df['target']\n","\n"," \n","  # Fit the label encoder to the tokens\n","  label_encoder.fit(y_train)\n","\n","  #y_all = np.concatenate([y_train], axis=0)\n","\n","  # Fit the label encoder to the targets\n","  label_encoder.fit(y_train)\n","\n","\n","  # Encode training data y\n","  y_train = label_encoder.transform(y_train)\n","\n","\n","  #print(X_train)\n","  print(f'{len(set(y_train))}: Arguement categories')\n","\n","\n","  #df = df.assign(label=[0 if target==\"_\" or target==\"V\" else 1 for target in df['target']])\n","  #df= df.drop('target', axis=1)\n","  return y_train\n","\n","#clean column\n","def columns_cleaning(df): #political choices inside\n","  \"\"\"This function perform a preprocessing steps that consists of:\n","  - in removing rows with missinvg value for distance head feature\n","  - cast numerical features to int \n","  - drop token_id (I belive is useless)\n","  \n","\n","  \"\"\"\n","  df=df[df[\"distance_head\"]!=\"_\"]#like this\n","  df[\"distance_head\"]=df[\"distance_head\"].astype(int) #WARNING IS FROM HERE\n","  \n","  df[\"sent_id\"]=df[\"sent_id\"].astype(int) #WARNING IS FROM HERE\n","\n","  df= df.drop('token_id', axis=1) #or this\n","  df=df[['sent_id', 'repetion_id','token', 'lemma', 'POS', 'uni_POS', 'morph_type',\n","        'distance_head', 'dep_label', 'dep_rel', 'space', 'probbank' ,'label']]\n","\n","  return df\n","\n","\n","def make_NER(df):\n","  return df\n","\n","\n","\"\"\"cols_to_encode=[ 'token', 'lemma', 'POS', 'uni_POS',\n","       'morph_type', 'dep_label', 'dep_rel', 'space',\n","       'probbank']\"\"\"\n","\n","\"\"\"cols_to_encode=[ 'POS', 'uni_POS',\n","       'morph_type', 'dep_label', 'dep_rel', 'space',\n","       'probbank']\"\"\"\n","\n","\n","def create_encoding(train_df,test_df,cols_to_encode):\n","  \"\"\"This function creates a label encoding (just assign number to every value) for all columns in the list cols_to_encode \"\"\"\n","  # create a LabelEncoder objec\n","  oe = OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1)\n","  train_df=oe.fit_transform(df_train[cols_to_encode])\n","  test_df = oe.transform(df_test[cols_to_encode])\n","  #print(oe.categories_)\n","  # iterate over the columns to encode\n","  \"\"\"for col in cols_to_encode:\n","      train_df[col] = oe.fit_transform(train_df[col])\n","      test_df[col] = oe.transform(test_df[col])\n","  \"\"\"\n","  return train_df, test_df\n"],"metadata":{"id":"_yQNwyyKqQge"},"execution_count":null,"outputs":[],"id":"_yQNwyyKqQge"},{"cell_type":"code","source":["def prepare_token_data(tokens_train, tokens_test):\n","\n","  encoder = LabelEncoder()\n","  vectorizer = CountVectorizer()\n","\n","  X_train = vectorizer.fit_transform(tokens_train['token'])\n","  X_test = vectorizer.transform(tokens_test['token'])\n","  \n","\n","  y_train = tokens_train['target']\n","  y_test = tokens_test['target']\n","  y_all = np.concatenate([y_train, y_test], axis=0)\n","\n","\n","  # Fit the label encoder to the targets\n","  encoder.fit(y_all.astype(str))\n","\n","  \n","  y_train = encoder.transform(y_train.astype(str))\n","  y_test = encoder.transform(y_test.astype(str))\n","\n","\n","\n","\n","  return X_train, X_test, y_train, y_test, encoder\n"],"metadata":{"id":"PKSGRSntrGPM"},"id":"PKSGRSntrGPM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def logistic_reg(X_train, X_test, y_train, y_test):\n","  #instantiate the model\n","  log_regression = LogisticRegression(penalty='l2')\n","\n","  #fit the model using the training data\n","  log_regression.fit(X_train, y_train)\n","\n","  #use model to make predictions on test data\n","  y_pred = log_regression.predict(X_test)\n","  return y_pred"],"metadata":{"id":"uznV_TMnrMSY"},"execution_count":null,"outputs":[],"id":"uznV_TMnrMSY"},{"cell_type":"code","source":["def confusion_mtrx(y_test, y_pred):\n","\n","  # get all categories of LabelEncoder in list\n","  categories = list(set(y_test))\n","\n","  size = len(categories)\n","\n","  cm = confusion_matrix(y_test, y_pred)\n","  ax= plt.subplot()\n","\n","  #ax.rcParams['figure.figsize'] = [3.8,3.8]\n","\n","  sns.heatmap(cm, annot=True, fmt='g', ax=ax,cmap='Blues');  #annot=True to annotate cells, ftm='g' to disable scientific notation\n","  sns.color_palette(\"tab10\")  \n","  \n","\n","\n","  # labels, title and ticks\n","  ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n","  ax.set_title('Confusion Matrix'); \n","  ax.xaxis.set_ticklabels(categories); ax.yaxis.set_ticklabels(categories);\n","  return 0"],"metadata":{"id":"u1Ptk9F_rjjf"},"execution_count":null,"outputs":[],"id":"u1Ptk9F_rjjf"},{"cell_type":"code","source":["tokens_train = train[[\"token\", \"target\"]]\n","tokens_test = test[[\"token\", \"target\"]]\n","\n","\n","X_train, X_test, y_train, y_test, encoder = prepare_token_data(tokens_train, tokens_test)\n","\n","# reshape data so they fit to model\n","#X_train = X_train.reshape(-1, 1)\n","#X_test = X_test.reshape(-1, 1)\n","\n","print(X_train.shape)\n","print(y_train.shape)\n","\n","\n","print(f'\\n{X_test.shape}')\n","print(y_test.shape)\n","\n","y_pred = logistic_reg(X_train, X_test, y_train, y_test)\n","f1 = f1_score(y_test, y_pred, average='micro')\n","print(f\"\\n{f1=}\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","\n","\n","# Decode the predicted target values\n","y_test = encoder.inverse_transform(y_test)\n","\n","# Decode the actual target values\n","y_pred = encoder.inverse_transform(y_pred)"],"metadata":{"id":"DNCxgvxArMFI"},"id":"DNCxgvxArMFI","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"colab":{"provenance":[{"file_id":"1R-cmPC85Grk9XAqB-oYGGe_fXmFZIJOZ","timestamp":1677266299601},{"file_id":"https://github.com/hasan-sh/advanced-nlp/blob/main/Firstmodel_baseline.ipynb","timestamp":1677264143789}],"collapsed_sections":["dwtocEY5W47H","xIjLhGYzgw35"]}},"nbformat":4,"nbformat_minor":5}